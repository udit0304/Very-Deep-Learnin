{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 1\n",
    "\n",
    "\n",
    "\n",
    "### Read the Dataset\n",
    "\n",
    "- Use Pandas to read the 'covertype.csv' file\n",
    "- The dataset contains information on different forest cover types\n",
    "- Look at the columns. Which of them contain meaningful features?\n",
    "\n",
    "\n",
    "\n",
    "### Seperate Features and Labels\n",
    "- Define x as the vectors of meaningful features\n",
    "- Define y as the labels (Cover_Type)\n",
    "\n",
    "\n",
    "\n",
    "### Split the dataset into two disjoint datasets for training and testing\n",
    "- Randomly split the dataset. Use 70% for training and 30% for testing.\n",
    "- Define x_train and x_test as the feature vectors\n",
    "- Define y_train and y_test as the labels\n",
    "    - Hint: Have a look at the sklearn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"covertype.csv\")\n",
    "df_columns=df.columns.tolist()\n",
    "x = df[df_columns[1:-1]]\n",
    "y = df[df_columns[-1]]\n",
    "\n",
    "x_train, x_test, y_train, y_test =  train_test_split(x, y, test_size=0.30, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a simple deep neural network\n",
    "- Use Keras to define a simple Multi-Layer Perceptron with at least 3 layers and a Softmax classifier\n",
    "    - You have to explicitly give the input shape of the first layer\n",
    "    - The other layer shapes are inferred\n",
    "    - The last layer should have as many neurons as there are classes\n",
    "        - How many classes are there?\n",
    "- Define 'accuracy' as performance metric when compiling the network model\n",
    "- Train the MLP with x_train, y_train\n",
    "    - Make sure to save the training history for later assessment\n",
    "- Evaluate the performance on x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9525 samples, validate on 1059 samples\n",
      "Epoch 1/100\n",
      "9525/9525 [==============================] - 4s 387us/step - loss: 13.8536 - acc: 0.1405 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 2/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 13.8252 - acc: 0.1423 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 3/100\n",
      "9525/9525 [==============================] - 1s 59us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 4/100\n",
      "9525/9525 [==============================] - 2s 163us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 5/100\n",
      "9525/9525 [==============================] - 1s 143us/step - loss: 13.8133 - acc: 0.1430 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 6/100\n",
      "9525/9525 [==============================] - 1s 63us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 7/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 13.8066 - acc: 0.1434 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 8/100\n",
      "9525/9525 [==============================] - 1s 64us/step - loss: 13.8133 - acc: 0.1430 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 9/100\n",
      "9525/9525 [==============================] - 1s 116us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 10/100\n",
      "9525/9525 [==============================] - 2s 199us/step - loss: 13.8116 - acc: 0.1431 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 11/100\n",
      "9525/9525 [==============================] - 1s 98us/step - loss: 13.8133 - acc: 0.1430 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 12/100\n",
      "9525/9525 [==============================] - 2s 212us/step - loss: 13.8133 - acc: 0.1430 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 13/100\n",
      "9525/9525 [==============================] - 1s 71us/step - loss: 13.8184 - acc: 0.1427 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 14/100\n",
      "9525/9525 [==============================] - 1s 68us/step - loss: 13.8201 - acc: 0.1426 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 15/100\n",
      "9525/9525 [==============================] - 2s 211us/step - loss: 13.8100 - acc: 0.1432 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 16/100\n",
      "9525/9525 [==============================] - 2s 179us/step - loss: 13.8184 - acc: 0.1427 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 17/100\n",
      "9525/9525 [==============================] - 1s 60us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 18/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 13.8235 - acc: 0.1424 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 19/100\n",
      "9525/9525 [==============================] - 2s 180us/step - loss: 13.8184 - acc: 0.1427 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 20/100\n",
      "9525/9525 [==============================] - 2s 210us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 21/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 22/100\n",
      "9525/9525 [==============================] - 1s 60us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 23/100\n",
      "9525/9525 [==============================] - 1s 129us/step - loss: 13.8201 - acc: 0.1426 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 24/100\n",
      "9525/9525 [==============================] - 2s 251us/step - loss: 13.8184 - acc: 0.1427 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 25/100\n",
      "9525/9525 [==============================] - 1s 60us/step - loss: 13.8116 - acc: 0.1431 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 26/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 27/100\n",
      "9525/9525 [==============================] - 1s 87us/step - loss: 13.8184 - acc: 0.1427 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 28/100\n",
      "9525/9525 [==============================] - 2s 249us/step - loss: 13.8201 - acc: 0.1426 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 29/100\n",
      "9525/9525 [==============================] - 1s 110us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 30/100\n",
      "9525/9525 [==============================] - 1s 62us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 31/100\n",
      "9525/9525 [==============================] - 1s 60us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 32/100\n",
      "9525/9525 [==============================] - 2s 232us/step - loss: 13.8184 - acc: 0.1427 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 33/100\n",
      "9525/9525 [==============================] - 1s 149us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 34/100\n",
      "9525/9525 [==============================] - 1s 62us/step - loss: 13.8218 - acc: 0.1425 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 35/100\n",
      "9525/9525 [==============================] - 1s 62us/step - loss: 13.8184 - acc: 0.1427 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 36/100\n",
      "9525/9525 [==============================] - 2s 194us/step - loss: 13.8218 - acc: 0.1425 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 37/100\n",
      "9525/9525 [==============================] - 2s 195us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 38/100\n",
      "9525/9525 [==============================] - 1s 63us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 39/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 40/100\n",
      "9525/9525 [==============================] - 2s 165us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 41/100\n",
      "9525/9525 [==============================] - 2s 217us/step - loss: 13.8184 - acc: 0.1427 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 42/100\n",
      "9525/9525 [==============================] - 1s 63us/step - loss: 13.8133 - acc: 0.1430 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 43/100\n",
      "9525/9525 [==============================] - 1s 64us/step - loss: 13.8184 - acc: 0.1427 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 44/100\n",
      "9525/9525 [==============================] - 1s 132us/step - loss: 13.8184 - acc: 0.1427 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 45/100\n",
      "9525/9525 [==============================] - 2s 257us/step - loss: 13.8184 - acc: 0.1427 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 46/100\n",
      "9525/9525 [==============================] - 1s 87us/step - loss: 13.8133 - acc: 0.1430 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 47/100\n",
      "9525/9525 [==============================] - 1s 124us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 48/100\n",
      "9525/9525 [==============================] - 3s 322us/step - loss: 13.8201 - acc: 0.1426 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 49/100\n",
      "9525/9525 [==============================] - 1s 87us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 50/100\n",
      "9525/9525 [==============================] - 1s 119us/step - loss: 13.8201 - acc: 0.1426 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 51/100\n",
      "9525/9525 [==============================] - 3s 312us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 52/100\n",
      "9525/9525 [==============================] - 1s 84us/step - loss: 13.8184 - acc: 0.1427 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 53/100\n",
      "9525/9525 [==============================] - 1s 77us/step - loss: 13.8218 - acc: 0.1425 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 54/100\n",
      "9525/9525 [==============================] - 2s 239us/step - loss: 13.8133 - acc: 0.1430 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 55/100\n",
      "9525/9525 [==============================] - 1s 153us/step - loss: 13.8201 - acc: 0.1426 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 56/100\n",
      "9525/9525 [==============================] - 1s 64us/step - loss: 13.8133 - acc: 0.1430 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 57/100\n",
      "9525/9525 [==============================] - 1s 80us/step - loss: 13.8133 - acc: 0.1430 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 58/100\n",
      "9525/9525 [==============================] - 2s 250us/step - loss: 13.8184 - acc: 0.1427 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9525/9525 [==============================] - 1s 123us/step - loss: 13.8184 - acc: 0.1427 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 60/100\n",
      "9525/9525 [==============================] - 1s 69us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 61/100\n",
      "9525/9525 [==============================] - 1s 106us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 62/100\n",
      "9525/9525 [==============================] - 3s 315us/step - loss: 13.8201 - acc: 0.1426 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 63/100\n",
      "9525/9525 [==============================] - 1s 77us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 64/100\n",
      "9525/9525 [==============================] - 1s 65us/step - loss: 13.8116 - acc: 0.1431 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 65/100\n",
      "9525/9525 [==============================] - 2s 248us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 66/100\n",
      "9525/9525 [==============================] - 1s 152us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 67/100\n",
      "9525/9525 [==============================] - 1s 63us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 68/100\n",
      "9525/9525 [==============================] - 1s 66us/step - loss: 13.8218 - acc: 0.1425 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 69/100\n",
      "9525/9525 [==============================] - 3s 300us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 70/100\n",
      "9525/9525 [==============================] - 3s 273us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 71/100\n",
      "9525/9525 [==============================] - 1s 114us/step - loss: 13.8116 - acc: 0.1431 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 72/100\n",
      "9525/9525 [==============================] - 4s 447us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 73/100\n",
      "9525/9525 [==============================] - 1s 95us/step - loss: 13.8116 - acc: 0.1431 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 74/100\n",
      "9525/9525 [==============================] - 1s 78us/step - loss: 13.8218 - acc: 0.1425 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 75/100\n",
      "9525/9525 [==============================] - 2s 161us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 76/100\n",
      "9525/9525 [==============================] - 2s 250us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 77/100\n",
      "9525/9525 [==============================] - 1s 67us/step - loss: 13.8133 - acc: 0.1430 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 78/100\n",
      "9525/9525 [==============================] - 2s 174us/step - loss: 13.8184 - acc: 0.1427 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 79/100\n",
      "9525/9525 [==============================] - 1s 114us/step - loss: 13.8184 - acc: 0.1427 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 80/100\n",
      "9525/9525 [==============================] - 2s 208us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 81/100\n",
      "9525/9525 [==============================] - 1s 68us/step - loss: 13.8201 - acc: 0.1426 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 82/100\n",
      "9525/9525 [==============================] - 1s 139us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 83/100\n",
      "9525/9525 [==============================] - 2s 220us/step - loss: 13.8184 - acc: 0.1427 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 84/100\n",
      "9525/9525 [==============================] - 1s 66us/step - loss: 13.8133 - acc: 0.1430 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 85/100\n",
      "9525/9525 [==============================] - 2s 178us/step - loss: 13.8201 - acc: 0.1426 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 86/100\n",
      "9525/9525 [==============================] - 1s 68us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 87/100\n",
      "9525/9525 [==============================] - 1s 154us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 88/100\n",
      "9525/9525 [==============================] - 1s 136us/step - loss: 13.8133 - acc: 0.1430 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 89/100\n",
      "9525/9525 [==============================] - 2s 210us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 90/100\n",
      "9525/9525 [==============================] - 1s 67us/step - loss: 13.8133 - acc: 0.1430 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 91/100\n",
      "9525/9525 [==============================] - 1s 113us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 92/100\n",
      "9525/9525 [==============================] - 1s 130us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 93/100\n",
      "9525/9525 [==============================] - 1s 68us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 94/100\n",
      "9525/9525 [==============================] - 2s 216us/step - loss: 13.8201 - acc: 0.1426 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 95/100\n",
      "9525/9525 [==============================] - 1s 104us/step - loss: 13.8116 - acc: 0.1431 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 96/100\n",
      "9525/9525 [==============================] - 2s 164us/step - loss: 13.8167 - acc: 0.1428 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 97/100\n",
      "9525/9525 [==============================] - 1s 66us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 98/100\n",
      "9525/9525 [==============================] - 2s 172us/step - loss: 13.8201 - acc: 0.1426 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 99/100\n",
      "9525/9525 [==============================] - 1s 69us/step - loss: 13.8133 - acc: 0.1430 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "Epoch 100/100\n",
      "9525/9525 [==============================] - 1s 156us/step - loss: 13.8150 - acc: 0.1429 - val_loss: 13.8807 - val_acc: 0.1388\n",
      "4536/4536 [==============================] - 1s 116us/step\n",
      "Test_loss: 13.801300 - Test_accuracy: 0.143739\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "input_dim = x_train.shape[1]\n",
    "number_of_classes = 8\n",
    "#input_dim\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_of_classes, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#multi-class classification problem\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "history= model.fit(x_train, y_train,\n",
    "          epochs=100,\n",
    "          batch_size=64,\n",
    "          validation_split=0.1)\n",
    "\n",
    "test_score = model.evaluate(x_test, y_test, batch_size=64)\n",
    "\n",
    "print(\"Test_loss: %f - Test_accuracy: %f\" % (test_score[0], test_score[1]))\n",
    "\n",
    "#print(\"Validation accuracy:\", history.history[\"val_acc\"])\n",
    "#print(\"Training accuracy:\", history.history[\"acc\"])\n",
    "\n",
    "\n",
    "\n",
    "# model.compile(...)\n",
    "\n",
    "# history = model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug\n",
    "- If your loss is NaN, either your network architecture or your data is faulty\n",
    "    - Check your network architecture\n",
    "    - Check your data\n",
    "        - Are there any NaN or infinite features or labels?\n",
    "    - Print the labels.\n",
    "        - How many unique labels do you have?\n",
    "        - Are they [0, ..., n-1]?\n",
    "            - If not, align them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train again\n",
    "- Reinitialize or redefine your MLP from above and train it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it work?\n",
    "- The loss should now be a number.\n",
    "- Does the network converge?\n",
    "\n",
    "\n",
    "\n",
    "### Inspect the data\n",
    "- Compute the min, max, mean and standard deviation of each feature\n",
    "- What data type do the columns have?\n",
    "- Use Pandas to print the statistics in a table\n",
    "- What could be problematic with the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Elevation</th>\n",
       "      <td>int64</td>\n",
       "      <td>1863</td>\n",
       "      <td>3849</td>\n",
       "      <td>2749.32</td>\n",
       "      <td>417.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspect</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>156.68</td>\n",
       "      <td>110.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slope</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>16.50</td>\n",
       "      <td>8.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1343</td>\n",
       "      <td>227.20</td>\n",
       "      <td>210.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <td>int64</td>\n",
       "      <td>-146</td>\n",
       "      <td>554</td>\n",
       "      <td>51.08</td>\n",
       "      <td>61.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>6890</td>\n",
       "      <td>1714.02</td>\n",
       "      <td>1325.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>212.70</td>\n",
       "      <td>30.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <td>int64</td>\n",
       "      <td>99</td>\n",
       "      <td>254</td>\n",
       "      <td>218.97</td>\n",
       "      <td>22.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>248</td>\n",
       "      <td>135.09</td>\n",
       "      <td>45.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>6993</td>\n",
       "      <td>1511.15</td>\n",
       "      <td>1099.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area1</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area2</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area3</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area4</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type1</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type2</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type3</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type4</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type5</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type6</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type7</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type8</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type9</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type10</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type11</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type12</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type13</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type14</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type15</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type16</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type17</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type18</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type19</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type20</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type21</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type22</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type23</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type24</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type25</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type26</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type27</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type28</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type29</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type30</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type31</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type32</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type33</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type34</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type35</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type36</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type37</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type38</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type39</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type40</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Type   Min   Max    Mean     Std\n",
       "Elevation                           int64  1863  3849 2749.32  417.68\n",
       "Aspect                              int64     0   360  156.68  110.09\n",
       "Slope                               int64     0    52   16.50    8.45\n",
       "Horizontal_Distance_To_Hydrology    int64     0  1343  227.20  210.08\n",
       "Vertical_Distance_To_Hydrology      int64  -146   554   51.08   61.24\n",
       "Horizontal_Distance_To_Roadways     int64     0  6890 1714.02 1325.07\n",
       "Hillshade_9am                       int64     0   254  212.70   30.56\n",
       "Hillshade_Noon                      int64    99   254  218.97   22.80\n",
       "Hillshade_3pm                       int64     0   248  135.09   45.90\n",
       "Horizontal_Distance_To_Fire_Points  int64     0  6993 1511.15 1099.94\n",
       "Wilderness_Area1                    int64     0     1    0.24    0.43\n",
       "Wilderness_Area2                    int64     0     1    0.03    0.18\n",
       "Wilderness_Area3                    int64     0     1    0.42    0.49\n",
       "Wilderness_Area4                    int64     0     1    0.31    0.46\n",
       "Soil_Type1                          int64     0     1    0.02    0.15\n",
       "Soil_Type2                          int64     0     1    0.04    0.20\n",
       "Soil_Type3                          int64     0     1    0.06    0.24\n",
       "Soil_Type4                          int64     0     1    0.06    0.23\n",
       "Soil_Type5                          int64     0     1    0.01    0.10\n",
       "Soil_Type6                          int64     0     1    0.04    0.20\n",
       "Soil_Type7                          int64     0     0    0.00    0.00\n",
       "Soil_Type8                          int64     0     1    0.00    0.01\n",
       "Soil_Type9                          int64     0     1    0.00    0.03\n",
       "Soil_Type10                         int64     0     1    0.14    0.35\n",
       "Soil_Type11                         int64     0     1    0.03    0.16\n",
       "Soil_Type12                         int64     0     1    0.02    0.12\n",
       "Soil_Type13                         int64     0     1    0.03    0.17\n",
       "Soil_Type14                         int64     0     1    0.01    0.11\n",
       "Soil_Type15                         int64     0     0    0.00    0.00\n",
       "Soil_Type16                         int64     0     1    0.01    0.09\n",
       "Soil_Type17                         int64     0     1    0.04    0.20\n",
       "Soil_Type18                         int64     0     1    0.00    0.06\n",
       "Soil_Type19                         int64     0     1    0.00    0.06\n",
       "Soil_Type20                         int64     0     1    0.01    0.10\n",
       "Soil_Type21                         int64     0     1    0.00    0.03\n",
       "Soil_Type22                         int64     0     1    0.02    0.15\n",
       "Soil_Type23                         int64     0     1    0.05    0.22\n",
       "Soil_Type24                         int64     0     1    0.02    0.13\n",
       "Soil_Type25                         int64     0     1    0.00    0.01\n",
       "Soil_Type26                         int64     0     1    0.00    0.06\n",
       "Soil_Type27                         int64     0     1    0.00    0.03\n",
       "Soil_Type28                         int64     0     1    0.00    0.02\n",
       "Soil_Type29                         int64     0     1    0.09    0.28\n",
       "Soil_Type30                         int64     0     1    0.05    0.21\n",
       "Soil_Type31                         int64     0     1    0.02    0.15\n",
       "Soil_Type32                         int64     0     1    0.05    0.21\n",
       "Soil_Type33                         int64     0     1    0.04    0.20\n",
       "Soil_Type34                         int64     0     1    0.00    0.04\n",
       "Soil_Type35                         int64     0     1    0.01    0.08\n",
       "Soil_Type36                         int64     0     1    0.00    0.03\n",
       "Soil_Type37                         int64     0     1    0.00    0.05\n",
       "Soil_Type38                         int64     0     1    0.05    0.21\n",
       "Soil_Type39                         int64     0     1    0.04    0.20\n",
       "Soil_Type40                         int64     0     1    0.03    0.17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "\n",
    "stats = pd.DataFrame(columns=[\"Type\", \"Min\", \"Max\", \"Mean\", \"Std\"])\n",
    "#x_train, x_test, y_train, y_test =  train_test_split(x, y, test_size=0.30, random_state=42)\n",
    "for col in x.columns:\n",
    "    stats.loc[col] = {\"Type\":x[col].dtype,\n",
    "                      \"Min\": x[col].min(),\n",
    "                      \"Max\": x[col].max(),\n",
    "                      \"Mean\": x[col].mean(),\n",
    "                      \"Std\":x[col].std() \n",
    "                 }\n",
    "\n",
    "\n",
    "\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Data\n",
    "- Normalize or standardize your data, so all features are at the same scale.\n",
    "    - This will help your network to use all available features and not be biased by some features with large values\n",
    "    - Does it make sense to normalize all columns, or only some?\n",
    "- Hint: Again, look if you find something useful in sklearn\n",
    "\n",
    "\n",
    "- Never use test data to optimize your training! This includes the preprocessing\n",
    "    - Find preprocessing parameters on your training data only!\n",
    "    - Transform all your data with the computed parameters\n",
    "    - You have to remember which of your samples are used for training and which are for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\udit0\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\udit0\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type31</th>\n",
       "      <th>Soil_Type32</th>\n",
       "      <th>Soil_Type33</th>\n",
       "      <th>Soil_Type34</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>-1.60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.28</td>\n",
       "      <td>4.33</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.35</td>\n",
       "      <td>4.29</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.83</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>4.19</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>4.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.37</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.32</td>\n",
       "      <td>4.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0      -0.37   -0.96  -1.60                              0.15   \n",
       "1      -0.38   -0.91  -1.72                             -0.07   \n",
       "2       0.13   -0.16  -0.89                              0.19   \n",
       "3       0.09   -0.02   0.18                              0.07   \n",
       "4      -0.37   -1.01  -1.72                             -0.35   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                           -0.83                            -0.91   \n",
       "1                           -0.93                            -1.00   \n",
       "2                            0.23                             1.11   \n",
       "3                            1.09                             1.04   \n",
       "4                           -0.85                            -1.00   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0           0.27            0.57           0.28   \n",
       "1           0.24            0.70           0.35   \n",
       "2           0.70            0.83          -0.00   \n",
       "3           0.83            0.83          -0.29   \n",
       "4           0.24            0.66           0.32   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points     ...       Soil_Type31  Soil_Type32  \\\n",
       "0                                4.33     ...             -0.15        -0.22   \n",
       "1                                4.29     ...             -0.15        -0.22   \n",
       "2                                4.19     ...             -0.15        -0.22   \n",
       "3                                4.27     ...             -0.15        -0.22   \n",
       "4                                4.24     ...             -0.15        -0.22   \n",
       "\n",
       "   Soil_Type33  Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  \\\n",
       "0        -0.21        -0.04        -0.08        -0.03        -0.05   \n",
       "1        -0.21        -0.04        -0.08        -0.03        -0.05   \n",
       "2        -0.21        -0.04        -0.08        -0.03        -0.05   \n",
       "3        -0.21        -0.04        -0.08        -0.03        -0.05   \n",
       "4        -0.21        -0.04        -0.08        -0.03        -0.05   \n",
       "\n",
       "   Soil_Type38  Soil_Type39  Soil_Type40  \n",
       "0        -0.22        -0.21        -0.18  \n",
       "1        -0.22        -0.21        -0.18  \n",
       "2        -0.22        -0.21        -0.18  \n",
       "3        -0.22        -0.21        -0.18  \n",
       "4        -0.22        -0.21        -0.18  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Create the Scaler object\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Fit data on the scaler object\n",
    "scaled_df = scaler.fit_transform(x)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=x.columns)\n",
    "scaled_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data again\n",
    "- Print the statistics of the preprocessed data using the code from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Elevation</th>\n",
       "      <td>float64</td>\n",
       "      <td>-2.12</td>\n",
       "      <td>2.63</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspect</th>\n",
       "      <td>float64</td>\n",
       "      <td>-1.42</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slope</th>\n",
       "      <td>float64</td>\n",
       "      <td>-1.95</td>\n",
       "      <td>4.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <td>float64</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>5.31</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <td>float64</td>\n",
       "      <td>-3.22</td>\n",
       "      <td>8.21</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <td>float64</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>3.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <td>float64</td>\n",
       "      <td>-6.96</td>\n",
       "      <td>1.35</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <td>float64</td>\n",
       "      <td>-5.26</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <td>float64</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <td>float64</td>\n",
       "      <td>-1.37</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area1</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>1.79</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area2</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>5.41</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area3</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area4</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type1</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>6.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type2</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>4.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type3</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>3.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type4</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>4.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type5</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>9.52</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type6</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>4.72</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type7</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type8</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>122.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type9</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>38.87</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type10</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type11</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>6.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type12</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>8.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type13</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>5.55</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type14</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>9.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type15</th>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type16</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>11.47</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type17</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>4.87</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type18</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>15.84</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type19</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type20</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>10.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type21</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>30.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type22</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>6.54</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type23</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>4.36</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type24</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>7.60</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type25</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>122.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type26</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>16.70</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type27</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>31.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type28</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>40.98</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type29</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>3.27</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type30</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>4.46</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type31</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>6.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type32</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>4.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type33</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>4.85</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type34</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>26.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type35</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>12.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type36</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>38.87</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type37</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>21.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type38</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>4.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type39</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type40</th>\n",
       "      <td>float64</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>5.65</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Type   Min    Max  Mean  Std\n",
       "Elevation                           float64 -2.12   2.63 -0.00 1.00\n",
       "Aspect                              float64 -1.42   1.85  0.00 1.00\n",
       "Slope                               float64 -1.95   4.20  0.00 1.00\n",
       "Horizontal_Distance_To_Hydrology    float64 -1.08   5.31 -0.00 1.00\n",
       "Vertical_Distance_To_Hydrology      float64 -3.22   8.21 -0.00 1.00\n",
       "Horizontal_Distance_To_Roadways     float64 -1.29   3.91  0.00 1.00\n",
       "Hillshade_9am                       float64 -6.96   1.35 -0.00 1.00\n",
       "Hillshade_Noon                      float64 -5.26   1.54  0.00 1.00\n",
       "Hillshade_3pm                       float64 -2.94   2.46  0.00 1.00\n",
       "Horizontal_Distance_To_Fire_Points  float64 -1.37   4.98  0.00 1.00\n",
       "Wilderness_Area1                    float64 -0.56   1.79 -0.00 1.00\n",
       "Wilderness_Area2                    float64 -0.18   5.41 -0.00 1.00\n",
       "Wilderness_Area3                    float64 -0.85   1.18  0.00 1.00\n",
       "Wilderness_Area4                    float64 -0.67   1.49  0.00 1.00\n",
       "Soil_Type1                          float64 -0.16   6.45  0.00 1.00\n",
       "Soil_Type2                          float64 -0.21   4.82  0.00 1.00\n",
       "Soil_Type3                          float64 -0.26   3.84  0.00 1.00\n",
       "Soil_Type4                          float64 -0.24   4.12  0.00 1.00\n",
       "Soil_Type5                          float64 -0.11   9.52 -0.00 1.00\n",
       "Soil_Type6                          float64 -0.21   4.72 -0.00 1.00\n",
       "Soil_Type7                          float64  0.00   0.00  0.00 0.00\n",
       "Soil_Type8                          float64 -0.01 122.96  0.00 1.00\n",
       "Soil_Type9                          float64 -0.03  38.87  0.00 1.00\n",
       "Soil_Type10                         float64 -0.41   2.46  0.00 1.00\n",
       "Soil_Type11                         float64 -0.17   6.02  0.00 1.00\n",
       "Soil_Type12                         float64 -0.12   8.10  0.00 1.00\n",
       "Soil_Type13                         float64 -0.18   5.55 -0.00 1.00\n",
       "Soil_Type14                         float64 -0.11   9.41  0.00 1.00\n",
       "Soil_Type15                         float64  0.00   0.00  0.00 0.00\n",
       "Soil_Type16                         float64 -0.09  11.47 -0.00 1.00\n",
       "Soil_Type17                         float64 -0.21   4.87  0.00 1.00\n",
       "Soil_Type18                         float64 -0.06  15.84 -0.00 1.00\n",
       "Soil_Type19                         float64 -0.06  18.10  0.00 1.00\n",
       "Soil_Type20                         float64 -0.10  10.38  0.00 1.00\n",
       "Soil_Type21                         float64 -0.03  30.72  0.00 1.00\n",
       "Soil_Type22                         float64 -0.15   6.54 -0.00 1.00\n",
       "Soil_Type23                         float64 -0.23   4.36 -0.00 1.00\n",
       "Soil_Type24                         float64 -0.13   7.60 -0.00 1.00\n",
       "Soil_Type25                         float64 -0.01 122.96  0.00 1.00\n",
       "Soil_Type26                         float64 -0.06  16.70 -0.00 1.00\n",
       "Soil_Type27                         float64 -0.03  31.73  0.00 1.00\n",
       "Soil_Type28                         float64 -0.02  40.98 -0.00 1.00\n",
       "Soil_Type29                         float64 -0.31   3.27 -0.00 1.00\n",
       "Soil_Type30                         float64 -0.22   4.46 -0.00 1.00\n",
       "Soil_Type31                         float64 -0.15   6.67  0.00 1.00\n",
       "Soil_Type32                         float64 -0.22   4.57  0.00 1.00\n",
       "Soil_Type33                         float64 -0.21   4.85 -0.00 1.00\n",
       "Soil_Type34                         float64 -0.04  26.20  0.00 1.00\n",
       "Soil_Type35                         float64 -0.08  12.13  0.00 1.00\n",
       "Soil_Type36                         float64 -0.03  38.87  0.00 1.00\n",
       "Soil_Type37                         float64 -0.05  21.06  0.00 1.00\n",
       "Soil_Type38                         float64 -0.22   4.45  0.00 1.00\n",
       "Soil_Type39                         float64 -0.21   4.69  0.00 1.00\n",
       "Soil_Type40                         float64 -0.18   5.65 -0.00 1.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats = pd.DataFrame(columns=[\"Type\", \"Min\", \"Max\", \"Mean\", \"Std\"])\n",
    "\n",
    "for col in scaled_df.columns:\n",
    "    stats.loc[col] = {\"Type\": scaled_df[col].dtype,\n",
    "                      \"Min\": scaled_df[col].min(),\n",
    "                      \"Max\": scaled_df[col].max(),\n",
    "                      \"Mean\": scaled_df[col].mean(),\n",
    "                      \"Std\":scaled_df[col].std() \n",
    "                 }\n",
    "\n",
    "\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network again\n",
    "- Reinitialize or redefine your MLP from above and train it again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9525 samples, validate on 1059 samples\n",
      "Epoch 1/100\n",
      "9525/9525 [==============================] - 2s 235us/step - loss: 1.8068 - acc: 0.2770 - val_loss: 1.2134 - val_acc: 0.5817\n",
      "Epoch 2/100\n",
      "9525/9525 [==============================] - 2s 163us/step - loss: 1.3023 - acc: 0.4427 - val_loss: 1.0563 - val_acc: 0.5883\n",
      "Epoch 3/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 1.1708 - acc: 0.4952 - val_loss: 0.9690 - val_acc: 0.6015\n",
      "Epoch 4/100\n",
      "9525/9525 [==============================] - 2s 176us/step - loss: 1.1139 - acc: 0.5213 - val_loss: 0.9290 - val_acc: 0.6431\n",
      "Epoch 5/100\n",
      "9525/9525 [==============================] - 1s 102us/step - loss: 1.0480 - acc: 0.5482 - val_loss: 0.9050 - val_acc: 0.6327\n",
      "Epoch 6/100\n",
      "9525/9525 [==============================] - 1s 69us/step - loss: 1.0246 - acc: 0.5623 - val_loss: 0.8886 - val_acc: 0.6298\n",
      "Epoch 7/100\n",
      "9525/9525 [==============================] - 1s 60us/step - loss: 0.9949 - acc: 0.5783 - val_loss: 0.8436 - val_acc: 0.6648\n",
      "Epoch 8/100\n",
      "9525/9525 [==============================] - 1s 126us/step - loss: 0.9655 - acc: 0.5938 - val_loss: 0.8318 - val_acc: 0.6610\n",
      "Epoch 9/100\n",
      "9525/9525 [==============================] - 1s 142us/step - loss: 0.9697 - acc: 0.5939 - val_loss: 0.8135 - val_acc: 0.6714\n",
      "Epoch 10/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.9399 - acc: 0.6013 - val_loss: 0.8218 - val_acc: 0.6582\n",
      "Epoch 11/100\n",
      "9525/9525 [==============================] - 1s 60us/step - loss: 0.9204 - acc: 0.6140 - val_loss: 0.7860 - val_acc: 0.6799\n",
      "Epoch 12/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.9051 - acc: 0.6114 - val_loss: 0.7848 - val_acc: 0.6771\n",
      "Epoch 13/100\n",
      "9525/9525 [==============================] - 2s 202us/step - loss: 0.9008 - acc: 0.6261 - val_loss: 0.7728 - val_acc: 0.7073\n",
      "Epoch 14/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.8813 - acc: 0.6280 - val_loss: 0.7617 - val_acc: 0.6959\n",
      "Epoch 15/100\n",
      "9525/9525 [==============================] - 2s 197us/step - loss: 0.8879 - acc: 0.6349 - val_loss: 0.7688 - val_acc: 0.6865\n",
      "Epoch 16/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.8843 - acc: 0.6276 - val_loss: 0.7702 - val_acc: 0.6846\n",
      "Epoch 17/100\n",
      "9525/9525 [==============================] - 1s 60us/step - loss: 0.8642 - acc: 0.6438 - val_loss: 0.7660 - val_acc: 0.6922\n",
      "Epoch 18/100\n",
      "9525/9525 [==============================] - 2s 167us/step - loss: 0.8584 - acc: 0.6404 - val_loss: 0.7504 - val_acc: 0.7054\n",
      "Epoch 19/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.8586 - acc: 0.6429 - val_loss: 0.7465 - val_acc: 0.6959\n",
      "Epoch 20/100\n",
      "9525/9525 [==============================] - 2s 177us/step - loss: 0.8467 - acc: 0.6424 - val_loss: 0.7429 - val_acc: 0.6978\n",
      "Epoch 21/100\n",
      "9525/9525 [==============================] - 1s 108us/step - loss: 0.8365 - acc: 0.6466 - val_loss: 0.7640 - val_acc: 0.6988\n",
      "Epoch 22/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.8300 - acc: 0.6548 - val_loss: 0.7342 - val_acc: 0.7082\n",
      "Epoch 23/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.8362 - acc: 0.6493 - val_loss: 0.7514 - val_acc: 0.6771\n",
      "Epoch 24/100\n",
      "9525/9525 [==============================] - 1s 145us/step - loss: 0.8344 - acc: 0.6502 - val_loss: 0.7396 - val_acc: 0.6988\n",
      "Epoch 25/100\n",
      "9525/9525 [==============================] - 1s 129us/step - loss: 0.8331 - acc: 0.6525 - val_loss: 0.7289 - val_acc: 0.7262\n",
      "Epoch 26/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.8119 - acc: 0.6650 - val_loss: 0.7298 - val_acc: 0.7167\n",
      "Epoch 27/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.8183 - acc: 0.6560 - val_loss: 0.7216 - val_acc: 0.6997\n",
      "Epoch 28/100\n",
      "9525/9525 [==============================] - 1s 66us/step - loss: 0.8171 - acc: 0.6596 - val_loss: 0.7262 - val_acc: 0.7082\n",
      "Epoch 29/100\n",
      "9525/9525 [==============================] - 2s 202us/step - loss: 0.7959 - acc: 0.6708 - val_loss: 0.7103 - val_acc: 0.7243\n",
      "Epoch 30/100\n",
      "9525/9525 [==============================] - 1s 75us/step - loss: 0.8107 - acc: 0.6636 - val_loss: 0.7200 - val_acc: 0.7054\n",
      "Epoch 31/100\n",
      "9525/9525 [==============================] - 1s 71us/step - loss: 0.8052 - acc: 0.6644 - val_loss: 0.7031 - val_acc: 0.7063\n",
      "Epoch 32/100\n",
      "9525/9525 [==============================] - 1s 114us/step - loss: 0.8010 - acc: 0.6633 - val_loss: 0.7032 - val_acc: 0.7328\n",
      "Epoch 33/100\n",
      "9525/9525 [==============================] - 2s 169us/step - loss: 0.7989 - acc: 0.6695 - val_loss: 0.7029 - val_acc: 0.7129\n",
      "Epoch 34/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.8043 - acc: 0.6692 - val_loss: 0.7121 - val_acc: 0.7252\n",
      "Epoch 35/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.8118 - acc: 0.6605 - val_loss: 0.7006 - val_acc: 0.7280\n",
      "Epoch 36/100\n",
      "9525/9525 [==============================] - 1s 60us/step - loss: 0.7894 - acc: 0.6761 - val_loss: 0.7029 - val_acc: 0.7328\n",
      "Epoch 37/100\n",
      "9525/9525 [==============================] - 2s 210us/step - loss: 0.7930 - acc: 0.6720 - val_loss: 0.7171 - val_acc: 0.7186\n",
      "Epoch 38/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.7745 - acc: 0.6790 - val_loss: 0.7224 - val_acc: 0.7035\n",
      "Epoch 39/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.7896 - acc: 0.6681 - val_loss: 0.7167 - val_acc: 0.7262\n",
      "Epoch 40/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.7790 - acc: 0.6736 - val_loss: 0.7065 - val_acc: 0.7356\n",
      "Epoch 41/100\n",
      "9525/9525 [==============================] - 2s 159us/step - loss: 0.7769 - acc: 0.6778 - val_loss: 0.6910 - val_acc: 0.7167\n",
      "Epoch 42/100\n",
      "9525/9525 [==============================] - 1s 113us/step - loss: 0.7731 - acc: 0.6809 - val_loss: 0.6966 - val_acc: 0.7120\n",
      "Epoch 43/100\n",
      "9525/9525 [==============================] - 1s 62us/step - loss: 0.7831 - acc: 0.6787 - val_loss: 0.7017 - val_acc: 0.7016\n",
      "Epoch 44/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.7802 - acc: 0.6786 - val_loss: 0.6924 - val_acc: 0.7139\n",
      "Epoch 45/100\n",
      "9525/9525 [==============================] - 1s 91us/step - loss: 0.7805 - acc: 0.6744 - val_loss: 0.7108 - val_acc: 0.7092\n",
      "Epoch 46/100\n",
      "9525/9525 [==============================] - 2s 177us/step - loss: 0.7719 - acc: 0.6777 - val_loss: 0.6995 - val_acc: 0.7186\n",
      "Epoch 47/100\n",
      "9525/9525 [==============================] - 1s 62us/step - loss: 0.7729 - acc: 0.6817 - val_loss: 0.6826 - val_acc: 0.7214\n",
      "Epoch 48/100\n",
      "9525/9525 [==============================] - 1s 62us/step - loss: 0.7705 - acc: 0.6825 - val_loss: 0.6945 - val_acc: 0.7479\n",
      "Epoch 49/100\n",
      "9525/9525 [==============================] - 1s 62us/step - loss: 0.7743 - acc: 0.6820 - val_loss: 0.6795 - val_acc: 0.7271\n",
      "Epoch 50/100\n",
      "9525/9525 [==============================] - 2s 204us/step - loss: 0.7743 - acc: 0.6799 - val_loss: 0.6718 - val_acc: 0.7375\n",
      "Epoch 51/100\n",
      "9525/9525 [==============================] - 1s 64us/step - loss: 0.7605 - acc: 0.6827 - val_loss: 0.6759 - val_acc: 0.7365\n",
      "Epoch 52/100\n",
      "9525/9525 [==============================] - 2s 198us/step - loss: 0.7536 - acc: 0.6823 - val_loss: 0.6670 - val_acc: 0.7271\n",
      "Epoch 53/100\n",
      "9525/9525 [==============================] - 1s 62us/step - loss: 0.7652 - acc: 0.6805 - val_loss: 0.6780 - val_acc: 0.7139\n",
      "Epoch 54/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.7665 - acc: 0.6800 - val_loss: 0.6779 - val_acc: 0.7337\n",
      "Epoch 55/100\n",
      "9525/9525 [==============================] - 2s 162us/step - loss: 0.7730 - acc: 0.6854 - val_loss: 0.6763 - val_acc: 0.7177\n",
      "Epoch 56/100\n",
      "9525/9525 [==============================] - 1s 64us/step - loss: 0.7686 - acc: 0.6852 - val_loss: 0.6719 - val_acc: 0.7252\n",
      "Epoch 57/100\n",
      "9525/9525 [==============================] - 1s 152us/step - loss: 0.7540 - acc: 0.6864 - val_loss: 0.6590 - val_acc: 0.7460\n",
      "Epoch 58/100\n",
      "9525/9525 [==============================] - 1s 114us/step - loss: 0.7544 - acc: 0.6881 - val_loss: 0.6611 - val_acc: 0.7441\n",
      "Epoch 59/100\n",
      "9525/9525 [==============================] - 1s 62us/step - loss: 0.7595 - acc: 0.6841 - val_loss: 0.6677 - val_acc: 0.7262\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9525/9525 [==============================] - 1s 68us/step - loss: 0.7636 - acc: 0.6783 - val_loss: 0.6657 - val_acc: 0.7309\n",
      "Epoch 61/100\n",
      "9525/9525 [==============================] - 1s 86us/step - loss: 0.7520 - acc: 0.6908 - val_loss: 0.6617 - val_acc: 0.7158\n",
      "Epoch 62/100\n",
      "9525/9525 [==============================] - 2s 180us/step - loss: 0.7457 - acc: 0.6889 - val_loss: 0.6674 - val_acc: 0.7375\n",
      "Epoch 63/100\n",
      "9525/9525 [==============================] - 1s 65us/step - loss: 0.7572 - acc: 0.6828 - val_loss: 0.6641 - val_acc: 0.7328\n",
      "Epoch 64/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.7510 - acc: 0.6880 - val_loss: 0.6558 - val_acc: 0.7365\n",
      "Epoch 65/100\n",
      "9525/9525 [==============================] - 1s 60us/step - loss: 0.7517 - acc: 0.6881 - val_loss: 0.6576 - val_acc: 0.7432\n",
      "Epoch 66/100\n",
      "9525/9525 [==============================] - 2s 200us/step - loss: 0.7424 - acc: 0.6926 - val_loss: 0.6519 - val_acc: 0.7309\n",
      "Epoch 67/100\n",
      "9525/9525 [==============================] - 1s 63us/step - loss: 0.7470 - acc: 0.6932 - val_loss: 0.6485 - val_acc: 0.7441\n",
      "Epoch 68/100\n",
      "9525/9525 [==============================] - 1s 68us/step - loss: 0.7416 - acc: 0.6956 - val_loss: 0.6478 - val_acc: 0.7299\n",
      "Epoch 69/100\n",
      "9525/9525 [==============================] - 1s 62us/step - loss: 0.7405 - acc: 0.6910 - val_loss: 0.6333 - val_acc: 0.7554\n",
      "Epoch 70/100\n",
      "9525/9525 [==============================] - 1s 141us/step - loss: 0.7511 - acc: 0.6883 - val_loss: 0.6763 - val_acc: 0.7290\n",
      "Epoch 71/100\n",
      "9525/9525 [==============================] - 1s 146us/step - loss: 0.7393 - acc: 0.6995 - val_loss: 0.6834 - val_acc: 0.7384\n",
      "Epoch 72/100\n",
      "9525/9525 [==============================] - 1s 74us/step - loss: 0.7478 - acc: 0.6939 - val_loss: 0.6503 - val_acc: 0.7280\n",
      "Epoch 73/100\n",
      "9525/9525 [==============================] - 1s 72us/step - loss: 0.7479 - acc: 0.6929 - val_loss: 0.6640 - val_acc: 0.7120\n",
      "Epoch 74/100\n",
      "9525/9525 [==============================] - 2s 247us/step - loss: 0.7422 - acc: 0.6910 - val_loss: 0.6539 - val_acc: 0.7479\n",
      "Epoch 75/100\n",
      "9525/9525 [==============================] - 1s 74us/step - loss: 0.7414 - acc: 0.6933 - val_loss: 0.6477 - val_acc: 0.7450\n",
      "Epoch 76/100\n",
      "9525/9525 [==============================] - 2s 219us/step - loss: 0.7454 - acc: 0.6899 - val_loss: 0.6508 - val_acc: 0.7356\n",
      "Epoch 77/100\n",
      "9525/9525 [==============================] - 1s 61us/step - loss: 0.7249 - acc: 0.6946 - val_loss: 0.6491 - val_acc: 0.7356\n",
      "Epoch 78/100\n",
      "9525/9525 [==============================] - 1s 119us/step - loss: 0.7348 - acc: 0.6970 - val_loss: 0.6613 - val_acc: 0.7356\n",
      "Epoch 79/100\n",
      "9525/9525 [==============================] - 1s 119us/step - loss: 0.7387 - acc: 0.6959 - val_loss: 0.6463 - val_acc: 0.7413\n",
      "Epoch 80/100\n",
      "9525/9525 [==============================] - 1s 87us/step - loss: 0.7367 - acc: 0.6879 - val_loss: 0.6480 - val_acc: 0.7403\n",
      "Epoch 81/100\n",
      "9525/9525 [==============================] - 2s 198us/step - loss: 0.7354 - acc: 0.6969 - val_loss: 0.6594 - val_acc: 0.7564\n",
      "Epoch 82/100\n",
      "9525/9525 [==============================] - 2s 159us/step - loss: 0.7292 - acc: 0.7037 - val_loss: 0.6556 - val_acc: 0.7233\n",
      "Epoch 83/100\n",
      "9525/9525 [==============================] - 1s 112us/step - loss: 0.7374 - acc: 0.6964 - val_loss: 0.6506 - val_acc: 0.7403\n",
      "Epoch 84/100\n",
      "9525/9525 [==============================] - 1s 62us/step - loss: 0.7319 - acc: 0.6997 - val_loss: 0.6580 - val_acc: 0.7450\n",
      "Epoch 85/100\n",
      "9525/9525 [==============================] - 2s 179us/step - loss: 0.7480 - acc: 0.6885 - val_loss: 0.6395 - val_acc: 0.7498\n",
      "Epoch 86/100\n",
      "9525/9525 [==============================] - 1s 78us/step - loss: 0.7326 - acc: 0.6946 - val_loss: 0.6382 - val_acc: 0.7460\n",
      "Epoch 87/100\n",
      "9525/9525 [==============================] - 2s 212us/step - loss: 0.7307 - acc: 0.6995 - val_loss: 0.6719 - val_acc: 0.7243\n",
      "Epoch 88/100\n",
      "9525/9525 [==============================] - 1s 62us/step - loss: 0.7274 - acc: 0.7004 - val_loss: 0.6427 - val_acc: 0.7479\n",
      "Epoch 89/100\n",
      "9525/9525 [==============================] - 1s 66us/step - loss: 0.7280 - acc: 0.7011 - val_loss: 0.6560 - val_acc: 0.7309\n",
      "Epoch 90/100\n",
      "9525/9525 [==============================] - 1s 66us/step - loss: 0.7246 - acc: 0.7055 - val_loss: 0.6309 - val_acc: 0.7375\n",
      "Epoch 91/100\n",
      "9525/9525 [==============================] - 2s 189us/step - loss: 0.7263 - acc: 0.7050 - val_loss: 0.6447 - val_acc: 0.7432\n",
      "Epoch 92/100\n",
      "9525/9525 [==============================] - 1s 89us/step - loss: 0.7295 - acc: 0.6950 - val_loss: 0.6359 - val_acc: 0.7432\n",
      "Epoch 93/100\n",
      "9525/9525 [==============================] - 2s 219us/step - loss: 0.7333 - acc: 0.6932 - val_loss: 0.6365 - val_acc: 0.7526\n",
      "Epoch 94/100\n",
      "9525/9525 [==============================] - 1s 67us/step - loss: 0.7315 - acc: 0.6998 - val_loss: 0.6515 - val_acc: 0.7413\n",
      "Epoch 95/100\n",
      "9525/9525 [==============================] - 2s 169us/step - loss: 0.7240 - acc: 0.7035 - val_loss: 0.6291 - val_acc: 0.7488\n",
      "Epoch 96/100\n",
      "9525/9525 [==============================] - 1s 74us/step - loss: 0.7146 - acc: 0.7049 - val_loss: 0.6392 - val_acc: 0.7422\n",
      "Epoch 97/100\n",
      "9525/9525 [==============================] - 1s 122us/step - loss: 0.7405 - acc: 0.6952 - val_loss: 0.6363 - val_acc: 0.7488\n",
      "Epoch 98/100\n",
      "9525/9525 [==============================] - 1s 155us/step - loss: 0.7198 - acc: 0.6982 - val_loss: 0.6402 - val_acc: 0.7554\n",
      "Epoch 99/100\n",
      "9525/9525 [==============================] - 1s 66us/step - loss: 0.7290 - acc: 0.6998 - val_loss: 0.6341 - val_acc: 0.7507\n",
      "Epoch 100/100\n",
      "9525/9525 [==============================] - 1s 65us/step - loss: 0.7226 - acc: 0.7019 - val_loss: 0.6337 - val_acc: 0.7526\n",
      "4536/4536 [==============================] - 0s 25us/step\n",
      "Test_loss: 0.618467 - Test_accuracy: 0.743166\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "x_train, x_test, y_train, y_test =  train_test_split(scaled_df, y, test_size=0.30, random_state=42)\n",
    "input_dim = x_train.shape[1]\n",
    "number_of_classes = 8\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_of_classes, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#multi-class classification problem\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history= model.fit(x_train, y_train,\n",
    "          epochs=100,\n",
    "          batch_size=64, \n",
    "          validation_split=0.1)\n",
    "\n",
    "test_score = model.evaluate(x_test, y_test, batch_size=64)\n",
    "\n",
    "print(\"Test_loss: %f - Test_accuracy: %f\" % (test_score[0], test_score[1]))\n",
    "\n",
    "#print(\"Validation accuracy:\", history.history[\"val_acc\"])\n",
    "#print(\"Training accuracy:\", history.history[\"acc\"])\n",
    "\n",
    "# model.compile(...)\n",
    "\n",
    "# history = model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the training\n",
    "- use matplotlib.pyplot to visualize the keras history\n",
    "- plot both the training accuracy and the validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1871b9d9ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history[\"val_acc\"])\n",
    "plt.plot(history.history[\"acc\"])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Validation', 'Train'], loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
